{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # th∆∞ vi·ªán t√≠nh to√°n s·ªë h·ªçc\n",
    "import pandas as pd # th∆∞ vi·ªán gi√∫p ƒë·ªçc file v√† x·ª≠ l√≠ d·ªØ li·ªáu d·∫°ng b·∫£ng\n",
    "import yfinance as yf # th∆∞ vi·ªán l·∫•y d·ªØ li·ªáu\n",
    "import tensorflow as tf # th∆∞ vi·ªán model\n",
    "from tensorflow.keras.models import Sequential # S·∫Øp x·∫øp c√°c l·ªõp\n",
    "from tensorflow.keras.layers import Layer, LSTM, Dense, Dropout # C√°c l·ªõp s·ª≠ d·ª•ng trong m√¥ h√¨nh\n",
    "from sklearn.preprocessing import MinMaxScaler # Chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error # T√≠nh ƒë·ªô l·ªói\n",
    "import matplotlib.pyplot as plt # Th∆∞ vi·ªán v·∫Ω ƒë·ªì th·ªã\n",
    "import random # random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫£i d·ªØ li·ªáu 10 nƒÉm d√πng th∆∞ vi·ªán yfinance\n",
    "def download_stock_data(ticker):\n",
    "    data = yf.download(ticker, period=\"10y\", interval=\"1d\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• v·ªÅ 1 m√£ c·ªï phi·∫øu\n",
    "data = download_stock_data(\"GOOG\")\n",
    "# ƒê·∫£m b·∫£o d·ªØ li·ªáu l√† c·ªßa c√°c ng√†y li√™n t·ª•c ('D': daily),\n",
    "# c√°c ng√†y kh√¥ng c√≥ d·ªØ li·ªáu (T7,CN) th√¨ gi√° tr·ªã d·ªØ li·ªáu ƒë∆∞·ª£c g√°n NaN\n",
    "data = data.asfreq('D')\n",
    "\n",
    "# Ki·ªÉm tra 10 d√≤ng ƒë·∫ßu ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng s√≥t ng√†y n√†o\n",
    "data.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna() l√† h√†m d√πng ƒë·ªÉ ghi ƒë√® c√°c √¥ c√≥ gi√° tr·ªã NaN\n",
    "# method='ffill' (forward fill) nghƒ©a l√†: N·∫øu m·ªôt √¥ c√≥ gi√° tr·ªã NaN,\n",
    "# h√£y l·∫•y gi√° tr·ªã ·ªü d√≤ng ph√≠a tr√™n n√≥ ƒë·ªÉ ƒëi·ªÅn v√†o ()\n",
    "data = data.fillna(method='ffill')\n",
    "\n",
    "# Ki·ªÉm tra 10 d√≤ng ƒë·∫ßu ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≤n gi√° tr·ªã NaN n√†o.\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y ra duy nh·∫•t c·ªôt gi√° ƒë√≥ng c·ª≠a (Close) ‚Äì\n",
    "# ƒë√¢y l√† d·ªØ li·ªáu quan tr·ªçng nh·∫•t trong ph√¢n t√≠ch t√†i ch√≠nh v√† d·ª± b√°o.\n",
    "close_prices = data[['Close']]\n",
    "\n",
    "# Ki·ªÉm tra 10 gi√° tr·ªã ƒë·∫ßu c·ªßa chu·ªói Close, ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c l·ªçc ƒë√∫ng.\n",
    "close_prices.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5123e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V·∫Ω ƒë·ªì th·ªã gi√° th·ª±c\n",
    "dates = data.index[:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, close_prices, label='Actual Price')\n",
    "plt.title('Stock Price Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ef66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Build LSTM model using library\n",
    "def build_model(input_shape, units=50):\n",
    "    model = Sequential() # gi√∫p x·∫øp c√°c l·ªõp\n",
    "    model.add(LSTM(units, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def create_dataset(data, window_size=60): # k√≠ch th∆∞·ªõc c·ªßa s·ªï m·∫∑c ƒë·ªãnh(n·∫øu kh√¥ng truy·ªÅn v√†o th√¨ window_size=60)\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data)):\n",
    "        X.append(data[i-window_size:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y d·ªØ li·ªáu g·ªëc\n",
    "close_prices = data['Close'].values.reshape(-1, 1)\n",
    "total_len = len(close_prices)\n",
    "\n",
    "# Chia theo 80% train, 10% val, 10% test\n",
    "train_end = int(0.8 * total_len)\n",
    "val_end = int(0.9 * total_len)\n",
    "\n",
    "# ‚ö†Ô∏è Gi·ªØ l·∫°i 90 ng√†y tr∆∞·ªõc khi chia ƒë·ªÉ ƒë·ªß cho m·ªçi window_size\n",
    "max_window_size = 90\n",
    "train_raw = close_prices[:train_end]\n",
    "val_raw = close_prices[train_end - max_window_size:val_end]\n",
    "test_raw = close_prices[val_end - max_window_size:]\n",
    "\n",
    "# Chu·∫©n h√≥a\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_raw)\n",
    "val_scaled = scaler.transform(val_raw)\n",
    "test_scaled = scaler.transform(test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_swarm_optimization(population_size=10, max_generations=30,patience=5, target_mse=0.001, no_improve_count=0, w_max=0.9, w_min = 0.4):\n",
    "    # 1. T·∫°o c√° th·ªÉ ng·∫´u nhi√™n (t·ªça ƒë·ªô v√† v·∫≠n t·ªëc)\n",
    "    def create_particle():\n",
    "        position = {\n",
    "            'window_size': random.randint(30, 90),\n",
    "            'units': random.choice([64, 128])\n",
    "        }\n",
    "        velocity = {\n",
    "            'window_size': random.uniform(-10, 10),\n",
    "            'units': random.choice([-32, 0, 32])\n",
    "        }\n",
    "        return {\n",
    "            'position': position,\n",
    "            'velocity': velocity,\n",
    "            'best_position': position.copy(),\n",
    "            'best_score': float('inf')\n",
    "        }\n",
    "\n",
    "    # 2. T√≠nh fitness\n",
    "    def fitness(ind):\n",
    "        window_size = int(round(ind['position']['window_size']))\n",
    "        units = int(ind['position']['units'])\n",
    "\n",
    "        # Gi·ªõi h·∫°n window_size n·∫±m trong [30, 90] v√† units n·∫±m trong {32, 64, 128}\n",
    "        window_size = min(90, max(30, window_size))\n",
    "        units = min([64, 128], key=lambda x: abs(x - units))\n",
    "\n",
    "        # T·∫°o dataset t·ª´ t·∫≠p ƒë√£ chia & chu·∫©n h√≥a\n",
    "        X_train, y_train = create_dataset(train_scaled, window_size)\n",
    "        X_val, y_val = create_dataset(val_scaled, window_size)\n",
    "\n",
    "        model = build_model((window_size, 1), units=units)\n",
    "        model.fit(X_train, y_train, epochs=3, batch_size=32, verbose=0)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mse = mean_squared_error(y_val, preds)\n",
    "        return mse, window_size, units\n",
    "\n",
    "    # 3. Kh·ªüi t·∫°o qu·∫ßn th·ªÉ\n",
    "    swarm = [create_particle() for _ in range(population_size)]\n",
    "    global_best_position = None\n",
    "    global_best_score = float('inf')\n",
    "    best_score_history = float('inf')\n",
    "\n",
    "    for gen in range(max_generations):\n",
    "        print(f\"\\nLoop {gen+1}\")\n",
    "        w = w_max - (w_max - w_min) * gen / max_generations\n",
    "        c1 = 2.5 - 2 * (gen / max_generations)\n",
    "        c2 = 0.5 + 2 * (gen / max_generations)\n",
    "        for i, particle in enumerate(swarm):\n",
    "            score, actual_ws, actual_units = fitness(particle)\n",
    "            print(f\"Particle {i+1}: window_size={actual_ws}, units={actual_units}, MSE={score:.6f}\")\n",
    "\n",
    "            if score < particle['best_score']:\n",
    "                particle['best_score'] = score\n",
    "                particle['best_position'] = particle['position'].copy()\n",
    "\n",
    "            if score < global_best_score:\n",
    "                global_best_score = score\n",
    "                global_best_position = particle['position'].copy()\n",
    "\n",
    "        # 4. C·∫≠p nh·∫≠t v·∫≠n t·ªëc v√† v·ªã tr√≠\n",
    "        for particle in swarm:\n",
    "            for key in ['window_size', 'units']:\n",
    "                r1 = random.random()\n",
    "                r2 = random.random()\n",
    "                cognitive = c1 * r1 * (particle['best_position'][key] - particle['position'][key])\n",
    "                social = c2 * r2 * (global_best_position[key] - particle['position'][key])\n",
    "                particle['velocity'][key] = w * particle['velocity'][key] + cognitive + social\n",
    "                #particle['position'][key] += particle['velocity'][key]\n",
    "\n",
    "                # üëâ Velocity Clamping (Gi·ªõi h·∫°n v·∫≠n t·ªëc)\n",
    "                #v_max = {'window_size': 10, 'units': 32}\n",
    "                #particle['velocity'][key] = max(-v_max[key], min(v_max[key], particle['velocity'][key]))\n",
    "\n",
    "                particle['position'][key] += particle['velocity'][key]\n",
    "                # Gi·ªõi h·∫°n window_size v√† units\n",
    "                if key == 'window_size':\n",
    "                    particle['position'][key] = min(90, max(30, particle['position'][key]))\n",
    "                elif key == 'units':\n",
    "                    # Ch·ªâ cho ph√©p g·∫ßn c√°c gi√° tr·ªã 32, 64, 128\n",
    "                    raw_units = particle['position'][key]\n",
    "                    particle['position'][key] = min([64, 128], key=lambda x: abs(x - raw_units))\n",
    "                    \n",
    "        print(f\"Best individual of loop {gen+1}: window_size={int(global_best_position['window_size'])}, units={int(global_best_position['units'])}, MSE={global_best_score:.6f}\")\n",
    "\n",
    "         # üî∏ ƒêi·ªÅu ki·ªán d·ª´ng s·ªõm\n",
    "        if global_best_score < best_score_history:\n",
    "            best_score_history = global_best_score\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "\n",
    "        if global_best_score <= target_mse:\n",
    "            print(f\"Early stopping: ƒë·∫°t MSE m·ª•c ti√™u {target_mse} ·ªü th·∫ø h·ªá {gen+1}.\")\n",
    "            break\n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"Early stopping: kh√¥ng c·∫£i thi·ªán trong {patience} th·∫ø h·ªá.\")\n",
    "            break\n",
    "\n",
    "    # Tr·∫£ v·ªÅ k·∫øt qu·∫£ t·ªët nh·∫•t\n",
    "    final_window_size = int(round(global_best_position['window_size']))\n",
    "    final_units = int(global_best_position['units'])\n",
    "    print(f\"\\nFinal best individual: window_size={final_window_size}, units={final_units}, MSE={global_best_score:.6f}\")\n",
    "    return {'window_size': final_window_size, 'units': final_units}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # GA optimization\n",
    "best_params = particle_swarm_optimization()\n",
    "print(\"Best Parameters from PSO:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. L·∫•y d·ªØ li·ªáu g·ªëc (ch∆∞a chu·∫©n h√≥a)\n",
    "close_prices = data['Close'].values.reshape(-1, 1)\n",
    "\n",
    "# 2. Chia d·ªØ li·ªáu theo t·ªâ l·ªá 80% train, 10% val, 10% test\n",
    "total_len = len(close_prices)\n",
    "train_end = int(total_len * 0.8)\n",
    "val_end = int(total_len * 0.9)\n",
    "\n",
    "train_prices = close_prices[:train_end]\n",
    "val_prices = close_prices[train_end - best_params['window_size']:val_end]\n",
    "test_prices = close_prices[val_end - best_params['window_size']:]  # gi·ªØ l·∫°i window_size ng√†y\n",
    "\n",
    "# 3. Fit scaler tr√™n t·∫≠p train v√† transform c·∫£ 3 ph·∫ßn\n",
    "scaler = StandardScaler()\n",
    "scaled_train = scaler.fit_transform(train_prices)\n",
    "scaled_val = scaler.transform(val_prices)\n",
    "scaled_test = scaler.transform(test_prices)\n",
    "\n",
    "# 4. T·∫°o dataset\n",
    "X_train, y_train = create_dataset(scaled_train, best_params['window_size'])\n",
    "X_val, y_val = create_dataset(scaled_val, best_params['window_size'])\n",
    "X_test, y_test = create_dataset(scaled_test, best_params['window_size'])\n",
    "\n",
    "# 5. G·ªôp train + val ƒë·ªÉ train m√¥ h√¨nh cu·ªëi c√πng\n",
    "X_final_train = np.concatenate([X_train, X_val])\n",
    "y_final_train = np.concatenate([y_train, y_val])\n",
    "\n",
    "# 6. X√¢y m√¥ h√¨nh\n",
    "model = build_model((best_params['window_size'], 1), best_params['units'])\n",
    "\n",
    "# 7. EarlyStopping (monitor tr√™n `loss` v√¨ kh√¥ng c√≥ val)\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 8. Hu·∫•n luy·ªán tr√™n train + val\n",
    "model.fit(\n",
    "    X_final_train, y_final_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 9. D·ª± ƒëo√°n tr√™n test set v√† t√≠nh RMSE\n",
    "preds = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"‚úÖ Test RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb30861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D·ª± ƒëo√°n t·ª´ng ph·∫ßn\n",
    "train_preds = model.predict(X_train)\n",
    "val_preds = model.predict(X_val)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "# Inverse transform c√°c ph·∫ßn\n",
    "y_train_rescaled = scaler.inverse_transform(y_train)\n",
    "train_preds_rescaled = scaler.inverse_transform(train_preds)\n",
    "\n",
    "y_val_rescaled = scaler.inverse_transform(y_val)\n",
    "val_preds_rescaled = scaler.inverse_transform(val_preds)\n",
    "\n",
    "y_test_rescaled = scaler.inverse_transform(y_test)\n",
    "test_preds_rescaled = scaler.inverse_transform(test_preds)\n",
    "\n",
    "# L·∫•y index ng√†y t∆∞∆°ng ·ª©ng\n",
    "train_start = best_params['window_size']\n",
    "val_start = train_end\n",
    "test_start = val_end\n",
    "\n",
    "train_dates = data.index[train_start:train_start + len(y_train)]\n",
    "val_dates = data.index[val_start:val_start + len(y_val)]\n",
    "test_dates = data.index[test_start:test_start + len(y_test)]\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì chung\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(train_dates, y_train_rescaled, label='Actual Train', color='blue')\n",
    "#plt.plot(train_dates, train_preds_rescaled, label='Predicted Train', color='black', linestyle='--')\n",
    "\n",
    "plt.plot(val_dates, y_val_rescaled, label='Actual Val', color='orange')\n",
    "plt.plot(val_dates, val_preds_rescaled, label='Predicted Val', color='purple', linestyle='--')\n",
    "\n",
    "plt.plot(test_dates, y_test_rescaled, label='Actual Test', color='green')\n",
    "plt.plot(test_dates, test_preds_rescaled, label='Predicted Test', color='red', linestyle='--')\n",
    "\n",
    "plt.title('Stock Price Prediction vs Actual (Train, Val, Test)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0414a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D·ª± ƒëo√°n tr√™n t·∫≠p validation\n",
    "val_preds = model.predict(X_val)\n",
    "\n",
    "# Inverse transform y_val v√† d·ª± ƒëo√°n\n",
    "y_val_rescaled = scaler.inverse_transform(y_val)\n",
    "val_preds_rescaled = scaler.inverse_transform(val_preds)\n",
    "\n",
    "# MAE tr√™n t·∫≠p validation\n",
    "val_mae = np.mean(np.abs(y_val_rescaled - val_preds_rescaled))\n",
    "print(\"ƒê·ªô sai s·ªë trung b√¨nh tr√™n t·∫≠p validation:\", val_mae, \"dolar\")\n",
    "\n",
    "# L·∫•y index ng√†y ƒë√∫ng cho t·∫≠p validation\n",
    "val_start_index = train_end + best_params['window_size']\n",
    "val_dates = data.index[val_start_index : val_start_index + len(y_val)]\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(val_dates, y_val_rescaled, label='Actual Price', color='orange')\n",
    "plt.plot(val_dates, val_preds_rescaled, label='Predicted Price', color='purple', linestyle='--')\n",
    "plt.title('PSO Validation Set: Predicted vs Actual Stock Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "y_test_rescaled = scaler.inverse_transform(y_test)\n",
    "preds_rescaled = scaler.inverse_transform(preds)\n",
    "mae = np.mean(np.abs(y_test_rescaled- preds_rescaled))\n",
    "print(\"ƒê·ªô sai s·ªë trung b√¨nh tr√™n t·∫≠p test:\", mae, \"dolar\")\n",
    "mape = np.mean(np.abs((y_test_rescaled - preds_rescaled) / y_test_rescaled)) * 100 # T·ª∑ l·ªá ph·∫ßn trƒÉm sai s·ªë trung b√¨nh\n",
    "print(f\"MAPE tr√™n t·∫≠p test: {mape:.2f}%\")\n",
    "# Get the corresponding dates for the last 100 entries (the test set)\n",
    "dates = data.index[-len(y_test):]  # data is the original DataFrame from yfinance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, y_test_rescaled, label='Actual Price', color='green')\n",
    "plt.plot(dates, preds_rescaled, label='Predicted Price', color='red', linestyle='--')\n",
    "plt.title('PSO Test: Stock Price Prediction vs Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafd3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·ªâ l·∫•y 100 m·∫´u cu·ªëi c√πng\n",
    "num_plot = 100\n",
    "y_test_subset = y_test[-num_plot:]\n",
    "preds_subset = preds[-num_plot:]\n",
    "\n",
    "# Bi·∫øn ƒë·ªïi ng∆∞·ª£c l·∫°i gi√° g·ªëc\n",
    "y_test_rescaled = scaler.inverse_transform(y_test_subset)\n",
    "preds_rescaled = scaler.inverse_transform(preds_subset)\n",
    "\n",
    "# L·∫•y ƒë√∫ng 100 ng√†y cu·ªëi t∆∞∆°ng ·ª©ng trong d·ªØ li·ªáu g·ªëc\n",
    "dates = data.index[-num_plot:]\n",
    "\n",
    "# V·∫Ω ƒë·ªì th·ªã\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, y_test_rescaled, label='Actual Price', color='green')\n",
    "plt.plot(dates, preds_rescaled, label='Predicted Price', color='red', linestyle='--')\n",
    "plt.title('PSO Test: Stock Price Prediction vs Actual (Last 100 Days)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
