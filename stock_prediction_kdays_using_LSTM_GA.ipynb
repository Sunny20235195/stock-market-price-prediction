{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # th∆∞ vi·ªán t√≠nh to√°n s·ªë h·ªçc\n",
    "import pandas as pd # th∆∞ vi·ªán gi√∫p ƒë·ªçc file v√† x·ª≠ l√≠ d·ªØ li·ªáu d·∫°ng b·∫£ng\n",
    "import yfinance as yf # th∆∞ vi·ªán l·∫•y d·ªØ li·ªáu\n",
    "import tensorflow as tf # th∆∞ vi·ªán model\n",
    "from tensorflow.keras.models import Sequential # S·∫Øp x·∫øp c√°c l·ªõp\n",
    "from tensorflow.keras.layers import Layer, LSTM, Dense, Dropout # C√°c l·ªõp s·ª≠ d·ª•ng trong m√¥ h√¨nh\n",
    "from sklearn.preprocessing import MinMaxScaler # Chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "from sklearn.metrics import mean_squared_error # T√≠nh ƒë·ªô l·ªói\n",
    "import matplotlib.pyplot as plt # Th∆∞ vi·ªán v·∫Ω ƒë·ªì th·ªã\n",
    "import random # random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫£i d·ªØ li·ªáu 10 nƒÉm d√πng th∆∞ vi·ªán yfinance\n",
    "def download_stock_data(ticker):\n",
    "    data = yf.download(ticker, period=\"10y\", interval=\"1d\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07dbc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• v·ªÅ 1 m√£ c·ªï phi·∫øu\n",
    "data = download_stock_data(\"GOOG\")\n",
    "# ƒê·∫£m b·∫£o d·ªØ li·ªáu l√† c·ªßa c√°c ng√†y li√™n t·ª•c ('D': daily),\n",
    "# c√°c ng√†y kh√¥ng c√≥ d·ªØ li·ªáu (T7,CN) th√¨ gi√° tr·ªã d·ªØ li·ªáu ƒë∆∞·ª£c g√°n NaN\n",
    "data = data.asfreq('D')\n",
    "\n",
    "# Ki·ªÉm tra 10 d√≤ng ƒë·∫ßu ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng s√≥t ng√†y n√†o\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07443b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna() l√† h√†m d√πng ƒë·ªÉ ghi ƒë√® c√°c √¥ c√≥ gi√° tr·ªã NaN\n",
    "# method='ffill' (forward fill) nghƒ©a l√†: N·∫øu m·ªôt √¥ c√≥ gi√° tr·ªã NaN,\n",
    "# h√£y l·∫•y gi√° tr·ªã ·ªü d√≤ng ph√≠a tr√™n n√≥ ƒë·ªÉ ƒëi·ªÅn v√†o ()\n",
    "data = data.fillna(method='ffill')\n",
    "\n",
    "# Ki·ªÉm tra 10 d√≤ng ƒë·∫ßu ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≤n gi√° tr·ªã NaN n√†o.\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4160ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y ra duy nh·∫•t c·ªôt gi√° ƒë√≥ng c·ª≠a (Close) ‚Äì\n",
    "# ƒë√¢y l√† d·ªØ li·ªáu quan tr·ªçng nh·∫•t trong ph√¢n t√≠ch t√†i ch√≠nh v√† d·ª± b√°o.\n",
    "close_prices = data[['Close']]\n",
    "\n",
    "# Ki·ªÉm tra 10 gi√° tr·ªã ƒë·∫ßu c·ªßa chu·ªói Close, ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c l·ªçc ƒë√∫ng.\n",
    "close_prices.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V·∫Ω ƒë·ªì th·ªã gi√° th·ª±c\n",
    "dates = data.index[:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, close_prices, label='Actual Price')\n",
    "plt.title('Stock Price Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_loss(y_true, y_pred):\n",
    "    diff_true = y_true[:, 1:] - y_true[:, :-1]\n",
    "    diff_pred = y_pred[:, 1:] - y_pred[:, :-1]\n",
    "    return tf.reduce_mean(tf.maximum(0.0, -diff_true * diff_pred))  # penalize opposite direction\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    alpha = 0.4\n",
    "    beta = 0.2  # ph·∫°t underprediction\n",
    "    gamma = 0.3  # ph·∫°t overprediction\n",
    "\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    dir_loss = directional_loss(y_true, y_pred)\n",
    "\n",
    "    under_penalty = tf.reduce_mean(tf.nn.relu(y_true - y_pred))\n",
    "    over_penalty = tf.reduce_mean(tf.nn.relu(y_pred - y_true))\n",
    "\n",
    "    return mse + alpha * dir_loss + beta * under_penalty + gamma * over_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Build LSTM model using library\n",
    "def build_model(input_shape, units=50, output_steps=5):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(output_steps))  # Output shape = (batch_size, k)\n",
    "    model.compile(optimizer='adam', loss=combined_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def create_dataset(data, window_size=60, k=5):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data) - k + 1):  # ensure enough room for k steps\n",
    "        X.append(data[i - window_size:i])\n",
    "        y.append(data[i:i + k].flatten())  # output is a sequence of k steps\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y d·ªØ li·ªáu g·ªëc\n",
    "close_prices = data['Close'].values.reshape(-1, 1)\n",
    "total_len = len(close_prices)\n",
    "\n",
    "# Chia theo 80% train, 10% val, 10% test\n",
    "train_end = int(0.8 * total_len)\n",
    "val_end = int(0.9 * total_len)\n",
    "\n",
    "# ‚ö†Ô∏è Gi·ªØ l·∫°i 90 ng√†y tr∆∞·ªõc khi chia ƒë·ªÉ ƒë·ªß cho m·ªçi window_size\n",
    "max_window_size = 180\n",
    "train_raw = close_prices[:train_end]\n",
    "val_raw = close_prices[train_end - max_window_size:val_end]\n",
    "test_raw = close_prices[val_end - max_window_size:]\n",
    "\n",
    "# Chu·∫©n h√≥a\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_raw)\n",
    "val_scaled = scaler.transform(val_raw)\n",
    "test_scaled = scaler.transform(test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Genetic Algorithm for hyperparameter tuning\n",
    "def genetic_algorithm(population_size=20, max_generations=30, mutation_rate=0.1, crossover_rate=0.8, k=5, patience=3, no_improve_count = 0, target_mse=0.002): # C√°c th√¥ng s·ªë trong GA\n",
    "    # --------------------------\n",
    "    # 1. Create a random individual\n",
    "    def create_individual():\n",
    "        return {\n",
    "            'window_size': random.randint(30, 180), # K√≠ch th∆∞·ªõc c·ª≠a s·ªï ch·ªçn t·ª´ 30 ƒë·∫øn 90 ng√†y (c√≥ th·ªÉ thay ƒë·ªïi)\n",
    "            'units': random.choice([64, 128]) # S·ªë units tr√™n 1 l·ªõp LSTM. Ch·ªçn c√°c s·ªë n√†y l√† v√¨ GPU, TPU t√≠nh to√°n hi·ªáu qu·∫£ h∆°n v·ªõi c√°c gi√° tr·ªã unit l√† b·ªôi c·ªßa 32\n",
    "        }\n",
    "\n",
    "    # --------------------------\n",
    "    # 2. Evaluate fitness (lower MSE is better)\n",
    "    def fitness(ind):\n",
    "        #X_win, y_win = create_dataset(scaled_data, ind['window_size'])\n",
    "        #X_win, y_win = X_win[:-100], y_win[:-100]  # use training data # d√πng ph·∫ßn ƒë·∫ßu ƒë·ªÉ train\n",
    "        #X_val_gen, y_val_gen = X_win[-100:], y_win[-100:] # d√πng 100 d·ªØ li·ªáu cu·ªëi ƒë·ªÉ test\n",
    "        #split_index = int(len(X_win) * 0.8) # chia t·∫≠p train - test l√† 80 - 20\n",
    "        #X_train_gen, y_train_gen = X_win[:split_index], y_win[:split_index]\n",
    "        #X_val_gen, y_val_gen = X_win[split_index:], y_win[split_index:]\n",
    "\n",
    "        # 1. Chia tr∆∞·ªõc (tr√™n d·ªØ li·ªáu g·ªëc ch∆∞a scale)\n",
    "        # close_prices = data['Close'].values.reshape(-1, 1)\n",
    "        # train_size = int(len(close_prices) * 0.8)\n",
    "        # train_prices = close_prices[:train_size]\n",
    "        # val_prices = close_prices[train_size - ind['window_size']:]  # gi·ªØ l·∫°i `window_size` ng√†y ƒë·ªÉ ƒë·∫£m b·∫£o t·∫°o ƒë∆∞·ª£c sample\n",
    "\n",
    "        # # 2. Chu·∫©n h√≥a ri√™ng t·ª´ng ph·∫ßn\n",
    "        # scaler = MinMaxScaler()\n",
    "        # train_scaled = scaler.fit_transform(train_prices)\n",
    "        # val_scaled = scaler.transform(val_prices)\n",
    "\n",
    "        # # 3. T·∫°o dataset\n",
    "        # # Inside fitness()\n",
    "        # X_train_gen, y_train_gen = create_dataset(train_scaled, ind['window_size'], k)\n",
    "        # X_val_gen, y_val_gen = create_dataset(val_scaled, ind['window_size'], k)\n",
    "\n",
    "        # model = build_model((ind['window_size'], 1), units=ind['units'], output_steps=k)\n",
    "        # model.fit(X_train_gen, y_train_gen, epochs=3, batch_size=32, verbose=0)\n",
    "\n",
    "        # preds = model.predict(X_val_gen)\n",
    "        # return mean_squared_error(y_val_gen, preds)\n",
    "\n",
    "        window_size = ind['window_size']\n",
    "        units = ind['units']\n",
    "        # T·∫°o dataset t·ª´ t·∫≠p ƒë√£ chia & chu·∫©n h√≥a\n",
    "        X_train, y_train = create_dataset(train_scaled, window_size, k)\n",
    "        X_val, y_val = create_dataset(val_scaled, window_size, k)\n",
    "\n",
    "        model = build_model((window_size, 1), units=units, output_steps=k)\n",
    "        model.fit(X_train, y_train, epochs=3, batch_size=32, verbose=0)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mse = mean_squared_error(y_val, preds)\n",
    "        return mse\n",
    "\n",
    "\n",
    "    # --------------------------\n",
    "    # 3. Selection: sort by fitness (elitism: top N)\n",
    "    def selection(population, scores, num_elites=5):\n",
    "        sorted_pop = [x for _, x in sorted(zip(scores, population))]\n",
    "        return sorted_pop[:num_elites] # l·∫•y tr∆∞·ªõc 15 c√° th·ªÉ t·ªët nh·∫•t (fitness th·∫•p nh·∫•t)\n",
    "\n",
    "    # --------------------------\n",
    "    # 4. Crossover: combine 2 parents into 1 child\n",
    "    def crossover(parent1, parent2):\n",
    "        if random.random() < crossover_rate: # n·∫øu random ra < t·ªâ l·ªá lai gh√©p th√¨ cho lai.\n",
    "            return {\n",
    "                'window_size': random.choice([parent1['window_size'], parent2['window_size']]),\n",
    "                'units': random.choice([parent1['units'], parent2['units']])\n",
    "            }\n",
    "        else:\n",
    "            # No crossover, just clone one of the parents(n·∫øu ko th√¨ ko l√†m g√¨)\n",
    "            return random.choice([parent1, parent2]).copy()\n",
    "\n",
    "    # --------------------------\n",
    "    # 5. Mutation: randomly alter genes\n",
    "    def mutate(ind):\n",
    "        if random.random() < mutation_rate: # random ra nh·ªè h∆°n t·ªâ l·ªá ƒë·ªôt bi·∫øn th√¨ cho ƒë·ªôt bi·∫øn\n",
    "            ind['window_size'] = random.randint(30, 180)\n",
    "        if random.random() < mutation_rate:\n",
    "            ind['units'] = random.choice([64, 128])\n",
    "        return ind\n",
    "\n",
    "    # --------------------------\n",
    "    # 6. Replacement: create new generation from elites + offspring\n",
    "    def create_next_generation(elites, population, size, scores):\n",
    "        next_gen = elites[:]\n",
    "        sorted_pop = [x for _, x in sorted(zip(scores, population))]\n",
    "        while len(next_gen) < size:\n",
    "            parent1, parent2 = random.sample(sorted_pop[:10], 2) # ch·ªçn 4 c√° th·ªÉ t·ªët nh·∫•t xong ch·ªçn 2 b·ªë m·∫π. R·ªìi ti·∫øn h√†nh lai gh√©p, ƒë·ªôt bi·∫øn n·∫øu c√≥.\n",
    "            child = crossover(parent1, parent2) # lai gh√©p\n",
    "            child = mutate(child) # ƒë·ªôt bi·∫øn\n",
    "            next_gen.append(child) # th√™m v√†o th·∫ø h·ªá sau\n",
    "        return next_gen\n",
    "\n",
    "    # --------------------------\n",
    "    # 7. Repeat over generations\n",
    "    population = [create_individual() for _ in range(population_size)]\n",
    "    best_individual = None\n",
    "    best_score = float('inf')\n",
    "\n",
    "    for gen in range(max_generations):\n",
    "        print(f\"\\n Generation {gen+1}\")\n",
    "        scores = [fitness(ind) for ind in population]\n",
    "        for i, (ind, score) in enumerate(zip(population, scores)):\n",
    "            print(f\"Individual {i+1}: window_size={ind['window_size']}, units={ind['units']}, MSE={score:.6f}\")\n",
    "\n",
    "        elites = selection(population, scores, num_elites=5)\n",
    "        best_elite = elites[0]\n",
    "        best_elite_score = scores[population.index(best_elite)]  # l·∫•y ƒë√∫ng fitness ƒë√£ t√≠nh\n",
    "        print(f\"Best individual of generation {gen+1}: window_size={best_elite['window_size']}, units={best_elite['units']}, MSE={best_elite_score:.6f}\")\n",
    "        # üî∏ ƒêi·ªÅu ki·ªán d·ª´ng s·ªõm\n",
    "     \n",
    "        # Ki·ªÉm tra c·∫£i thi·ªán\n",
    "        if best_elite_score < best_score:\n",
    "            best_score = best_elite_score\n",
    "            best_individual = best_elite.copy()\n",
    "            no_improve_count = 0  # reset b·ªô ƒë·∫øm\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            print(f\"No improvement for {no_improve_count} generation(s)\")\n",
    "\n",
    "        # üî¥ D·ª™NG S·ªöM n·∫øu kh√¥ng c·∫£i thi·ªán trong `patience` th·∫ø h·ªá\n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"\\nEarly stopping: No improvement in the last {patience} generations.\")\n",
    "            break\n",
    "                # üî¥ D·ª´ng s·ªõm n·∫øu ƒë·∫°t MSE m·ª•c ti√™u\n",
    "        if best_elite_score <= target_mse:\n",
    "            print(f\"\\n‚úÖ Early stopping: Reached target MSE ‚â§ {target_mse}\")\n",
    "            break\n",
    "\n",
    "\n",
    "        population = create_next_generation(elites, population, population_size, scores) # t·∫°o th·∫ø h·ªá m·ªõi (g·ªìm 5 c√° th·ªÉ t·ªët nh·∫•t c·ªßa th·∫ø h·ªá tr∆∞·ªõc v√† nh·ªØng c√° th·ªÉ kh√°c ƒë∆∞·ª£c lai gh√©p, ƒë·ªôt bi·∫øn)\n",
    "\n",
    "    print(f\"\\n Final best individual: window_size={best_individual['window_size']}, units={best_individual['units']}, MSE={best_score:.6f}\")\n",
    "    return best_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GA optimization\n",
    "best_params = genetic_algorithm(k=5)\n",
    "print(\"Best Parameters from GA:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6794a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. L·∫•y d·ªØ li·ªáu g·ªëc (ch∆∞a chu·∫©n h√≥a)\n",
    "close_prices = data['Close'].values.reshape(-1, 1)\n",
    "\n",
    "# 2. Chia d·ªØ li·ªáu theo t·ªâ l·ªá 80% train, 10% val, 10% test\n",
    "total_len = len(close_prices)\n",
    "train_end = int(total_len * 0.8)\n",
    "val_end = int(total_len * 0.9)\n",
    "\n",
    "train_prices = close_prices[:train_end]\n",
    "val_prices = close_prices[train_end - best_params['window_size']:val_end]\n",
    "test_prices = close_prices[val_end - best_params['window_size']:]  # gi·ªØ l·∫°i window_size ng√†y\n",
    "\n",
    "# 3. Fit scaler tr√™n t·∫≠p train v√† transform c·∫£ 3 ph·∫ßn\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train = scaler.fit_transform(train_prices)\n",
    "scaled_val = scaler.transform(val_prices)\n",
    "scaled_test = scaler.transform(test_prices)\n",
    "\n",
    "# 4. T·∫°o dataset\n",
    "k=5\n",
    "X_train, y_train = create_dataset(scaled_train, best_params['window_size'], k)\n",
    "X_val, y_val = create_dataset(scaled_val, best_params['window_size'], k)\n",
    "X_test, y_test = create_dataset(scaled_test, best_params['window_size'], k)\n",
    "\n",
    "# 5. G·ªôp train + val ƒë·ªÉ train m√¥ h√¨nh cu·ªëi c√πng\n",
    "X_final_train = np.concatenate([X_train, X_val])\n",
    "y_final_train = np.concatenate([y_train, y_val])\n",
    "\n",
    "# 6. X√¢y m√¥ h√¨nh\n",
    "model = build_model((best_params['window_size'], 1), best_params['units'])\n",
    "\n",
    "# 7. EarlyStopping (monitor tr√™n `loss` v√¨ kh√¥ng c√≥ val)\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 8. Hu·∫•n luy·ªán tr√™n train + val\n",
    "model.fit(\n",
    "    X_final_train, y_final_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 9. D·ª± ƒëo√°n tr√™n test set v√† t√≠nh RMSE\n",
    "preds = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"‚úÖ Test RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de45cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "val_preds = model.predict(X_val)\n",
    "\n",
    "def plot_all_forecasts(data, scaler, best_params, y_val, val_preds, y_test, test_preds):\n",
    "    window_size = best_params['window_size']\n",
    "    k = y_val.shape[1]\n",
    "\n",
    "    total_len = len(data)\n",
    "    train_end = int(0.8 * total_len)\n",
    "    val_end = int(0.9 * total_len)\n",
    "\n",
    "    # --- 1. L·∫•y index g·ªëc ---\n",
    "    train_dates = data.index[:train_end]\n",
    "\n",
    "    val_start_index = train_end - window_size\n",
    "    test_start_index = val_end - window_size\n",
    "\n",
    "    # --- 2. Gi·∫£i scale ---\n",
    "    y_val_rescaled = scaler.inverse_transform(y_val.reshape(-1, 1)).reshape(y_val.shape)\n",
    "    val_preds_rescaled = scaler.inverse_transform(val_preds.reshape(-1, 1)).reshape(val_preds.shape)\n",
    "\n",
    "    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "    test_preds_rescaled = scaler.inverse_transform(test_preds.reshape(-1, 1)).reshape(test_preds.shape)\n",
    "\n",
    "    # --- 3. T·∫°o dict gi√° th·ª±c ---\n",
    "    def get_actual_dict(y_rescaled, start_idx):\n",
    "        actual_dict = defaultdict(list)\n",
    "        for i in range(len(y_rescaled)):\n",
    "            start = start_idx + i + window_size\n",
    "            for j in range(y_rescaled.shape[1]):\n",
    "                date_idx = start + j\n",
    "                if date_idx < len(data):\n",
    "                    date = data.index[date_idx]\n",
    "                    actual_dict[date].append(y_rescaled[i][j])\n",
    "        return actual_dict\n",
    "\n",
    "    actual_val = get_actual_dict(y_val_rescaled, val_start_index)\n",
    "    actual_test = get_actual_dict(y_test_rescaled, test_start_index)\n",
    "\n",
    "    # --- 4. D·ª± ƒëo√°n t·ª´ng b∆∞·ªõc ---\n",
    "    def get_pred_lines(preds_rescaled, start_idx):\n",
    "        pred_step_dict = [defaultdict(list) for _ in range(k)]\n",
    "        for i in range(len(preds_rescaled)):\n",
    "            start = start_idx + i + window_size\n",
    "            for j in range(k):\n",
    "                date_idx = start + j\n",
    "                if date_idx < len(data):\n",
    "                    date = data.index[date_idx]\n",
    "                    pred_step_dict[j][date].append(preds_rescaled[i][j])\n",
    "        pred_lines = []\n",
    "        for j in range(k):\n",
    "            common_dates = sorted(set(pred_step_dict[j].keys()))\n",
    "            pred_avg = np.array([np.mean(pred_step_dict[j][d]) for d in common_dates])\n",
    "            pred_lines.append((common_dates, pred_avg))\n",
    "        return pred_lines\n",
    "\n",
    "    val_pred_lines = get_pred_lines(val_preds_rescaled, val_start_index)\n",
    "    test_pred_lines = get_pred_lines(test_preds_rescaled, test_start_index)\n",
    "\n",
    "    # --- 5. Plot ---\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Train th·ª±c t·∫ø\n",
    "    plt.plot(train_dates, data['Close'][:train_end], label='Train Actual', color='gray', linewidth=1.5)\n",
    "\n",
    "    # Val th·ª±c t·∫ø\n",
    "    val_actual_dates = sorted(actual_val.keys())\n",
    "    val_actual_avg = np.array([np.mean(actual_val[d]) for d in val_actual_dates])\n",
    "    plt.plot(val_actual_dates, val_actual_avg, label='Val Actual', color='blue', linewidth=2)\n",
    "\n",
    "    # Test th·ª±c t·∫ø\n",
    "    test_actual_dates = sorted(actual_test.keys())\n",
    "    test_actual_avg = np.array([np.mean(actual_test[d]) for d in test_actual_dates])\n",
    "    plt.plot(test_actual_dates, test_actual_avg, label='Test Actual', color='black', linewidth=2)\n",
    "\n",
    "    # C√°c b∆∞·ªõc d·ª± ƒëo√°n val\n",
    "    colors = plt.cm.Blues(np.linspace(0.4, 0.9, k))\n",
    "    for j, (dates, preds_j) in enumerate(val_pred_lines):\n",
    "        plt.plot(dates, preds_j, label=f'Val Predicted t+{j+1}', color=colors[j], linestyle='--')\n",
    "\n",
    "    # C√°c b∆∞·ªõc d·ª± ƒëo√°n test\n",
    "    colors = plt.cm.Greens(np.linspace(0.4, 0.9, k))\n",
    "    for j, (dates, preds_j) in enumerate(test_pred_lines):\n",
    "        plt.plot(dates, preds_j, label=f'Test Predicted t+{j+1}', color=colors[j], linestyle='--')\n",
    "\n",
    "    plt.title(f'Stock Forecasting: Actual vs {k}-step Predictions')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_all_forecasts(data, scaler, best_params, y_val, val_preds, y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D·ª± ƒëo√°n tr√™n t·∫≠p validation\n",
    "val_preds = model.predict(X_val)\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Rescale val ground truth v√† d·ª± ƒëo√°n\n",
    "y_val_2d = y_val.reshape(-1, 1)\n",
    "val_preds_2d = val_preds.reshape(-1, 1)\n",
    "\n",
    "y_val_rescaled = scaler.inverse_transform(y_val_2d).reshape(y_val.shape)\n",
    "val_preds_rescaled = scaler.inverse_transform(val_preds_2d).reshape(val_preds.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. T·∫°o actual_dict t·ª´ y_val_rescaled\n",
    "actual_dict = defaultdict(list)\n",
    "val_start_index = len(data) - len(y_test) - len(y_val) - best_params['window_size']\n",
    "\n",
    "for i in range(len(y_val_rescaled)):\n",
    "    start = val_start_index + i + best_params['window_size']\n",
    "    for j in range(y_val_rescaled.shape[1]):\n",
    "        date_idx = start + j\n",
    "        if date_idx < len(data):\n",
    "            date = data.index[date_idx]\n",
    "            actual_dict[date].append(y_val_rescaled[i][j])\n",
    "\n",
    "actual_dates = sorted(actual_dict.keys())\n",
    "actual_avg = np.array([np.mean(actual_dict[d]) for d in actual_dates])\n",
    "\n",
    "# -------------------------------\n",
    "# 3. D·ª± ƒëo√°n t·ª´ng b∆∞·ªõc t+1, ..., t+k\n",
    "k = val_preds_rescaled.shape[1]\n",
    "pred_step_dict = [defaultdict(list) for _ in range(k)]\n",
    "\n",
    "for i in range(len(val_preds_rescaled)):\n",
    "    start = val_start_index + i + best_params['window_size']\n",
    "    for j in range(k):\n",
    "        date_idx = start + j\n",
    "        if date_idx < len(data):\n",
    "            date = data.index[date_idx]\n",
    "            pred_step_dict[j][date].append(val_preds_rescaled[i][j])\n",
    "\n",
    "# Trung b√¨nh t·ª´ng b∆∞·ªõc\n",
    "pred_lines = []\n",
    "step_mae = []\n",
    "for j in range(k):\n",
    "    common_dates = sorted(set(pred_step_dict[j].keys()) & set(actual_dict.keys()))\n",
    "    pred_avg = np.array([np.mean(pred_step_dict[j][d]) for d in common_dates])\n",
    "    actual_avg_step = np.array([np.mean(actual_dict[d]) for d in common_dates])\n",
    "    pred_lines.append((common_dates, pred_avg))\n",
    "    step_mae.append(np.mean(np.abs(pred_avg - actual_avg_step)))\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# ƒê∆∞·ªùng th·ª±c t·∫ø\n",
    "plt.plot(actual_dates, actual_avg, label='Actual Price', color='black', linewidth=2)\n",
    "\n",
    "# C√°c ƒë∆∞·ªùng d·ª± ƒëo√°n t+1 ƒë·∫øn t+k\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, k))\n",
    "for j in range(k):\n",
    "    dates, preds_j = pred_lines[j]\n",
    "    plt.plot(dates, preds_j, label=f'Predicted t+{j+1}', color=colors[j], alpha=0.7)\n",
    "\n",
    "plt.title(f'Many-to-Many Validation Forecasting: Actual vs {k} Prediction Steps')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape cho inverse transform\n",
    "y_test_2d = y_test.reshape(-1, 1)\n",
    "preds_2d = preds.reshape(-1, 1)\n",
    "\n",
    "y_test_rescaled = scaler.inverse_transform(y_test_2d).reshape(y_test.shape)\n",
    "preds_rescaled = scaler.inverse_transform(preds_2d).reshape(preds.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# Chu·∫©n b·ªã actual price (trung b√¨nh n·∫øu tr√πng)\n",
    "actual_dict = defaultdict(list)\n",
    "start_index = len(data) - len(y_test_rescaled) - best_params['window_size']\n",
    "\n",
    "for i in range(len(y_test_rescaled)):\n",
    "    start = start_index + i + best_params['window_size']\n",
    "    for j in range(y_test_rescaled.shape[1]):\n",
    "        date_idx = start + j\n",
    "        if date_idx < len(data):\n",
    "            date = data.index[date_idx]\n",
    "            actual_dict[date].append(y_test_rescaled[i][j])\n",
    "\n",
    "# Trung b√¨nh gi√° th·ª±c t·∫ø n·∫øu tr√πng\n",
    "actual_dates = sorted(actual_dict.keys())\n",
    "actual_avg = np.array([np.mean(actual_dict[d]) for d in actual_dates])\n",
    "\n",
    "# ----------------------------\n",
    "# D·ª± ƒëo√°n theo t·ª´ng b∆∞·ªõc t+1, t+2, ..., t+k\n",
    "k = preds_rescaled.shape[1]\n",
    "pred_step_dict = [defaultdict(list) for _ in range(k)]\n",
    "\n",
    "for i in range(len(preds_rescaled)):\n",
    "    start = start_index + i + best_params['window_size']\n",
    "    for j in range(k):\n",
    "        date_idx = start + j\n",
    "        if date_idx < len(data):\n",
    "            date = data.index[date_idx]\n",
    "            pred_step_dict[j][date].append(preds_rescaled[i][j])\n",
    "\n",
    "# Trung b√¨nh t·ª´ng b∆∞·ªõc + t√≠nh MAE\n",
    "pred_lines = []\n",
    "step_mae = []\n",
    "for j in range(k):\n",
    "    common_dates = sorted(set(pred_step_dict[j].keys()) & set(actual_dict.keys()))\n",
    "    pred_avg = np.array([np.mean(pred_step_dict[j][d]) for d in common_dates])\n",
    "    actual_avg_step = np.array([np.mean(actual_dict[d]) for d in common_dates])\n",
    "    pred_lines.append((common_dates, pred_avg))\n",
    "    step_mae.append(np.mean(np.abs(pred_avg - actual_avg_step)))\n",
    "\n",
    "# ----------------------------\n",
    "# Trung b√¨nh t·∫•t c·∫£ b∆∞·ªõc d·ª± ƒëo√°n\n",
    "combined_pred_dict = defaultdict(list)\n",
    "for j in range(k):\n",
    "    for date, values in pred_step_dict[j].items():\n",
    "        combined_pred_dict[date].extend(values)\n",
    "\n",
    "common_avg_dates = sorted(set(combined_pred_dict.keys()) & set(actual_dict.keys()))\n",
    "avg_pred_line = np.array([np.mean(combined_pred_dict[d]) for d in common_avg_dates])\n",
    "\n",
    "# ----------------------------\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# ƒê∆∞·ªùng gi√° th·ª±c t·∫ø\n",
    "plt.plot(actual_dates, actual_avg, label='Actual Price', linewidth=2)\n",
    "\n",
    "# C√°c ƒë∆∞·ªùng d·ª± ƒëo√°n t+1 ƒë·∫øn t+k\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, k))\n",
    "for j in range(k):\n",
    "    dates, preds_j = pred_lines[j]\n",
    "    plt.plot(dates, preds_j, label=f'Predicted t+{j+1}', color=colors[j], alpha=0.7, linewidth=2)\n",
    "\n",
    "# ƒê∆∞·ªùng trung b√¨nh c√°c b∆∞·ªõc d·ª± ƒëo√°n\n",
    "plt.plot(common_avg_dates, avg_pred_line, label='Average Prediction', color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.title(f'Many-to-Many Forecasting: Actual vs {k} Prediction Steps')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≠nh MAE v√† MAPE cho ƒë∆∞·ªùng d·ª± ƒëo√°n trung b√¨nh\n",
    "common_actual_avg = np.array([np.mean(actual_dict[d]) for d in common_avg_dates])\n",
    "\n",
    "# MAE: sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh\n",
    "avg_mae = np.mean(np.abs(avg_pred_line - common_actual_avg))\n",
    "\n",
    "# MAPE: sai s·ªë ph·∫ßn trƒÉm tuy·ªát ƒë·ªëi trung b√¨nh\n",
    "# Tr√°nh chia cho 0 b·∫±ng c√°ch th√™m epsilon nh·ªè\n",
    "epsilon = 1e-8\n",
    "avg_mape = np.mean(np.abs((common_actual_avg - avg_pred_line) / (common_actual_avg + epsilon))) * 100\n",
    "\n",
    "print(f\"\\n‚û°Ô∏è  MAE (Average Prediction): {avg_mae:.4f} USD\")\n",
    "print(f\"‚û°Ô∏è  MAPE (Average Prediction): {avg_mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# preds, y_test ƒë√£ rescale r·ªìi (preds_rescaled, y_test_rescaled)\n",
    "# Gi·∫£ s·ª≠: preds_rescaled.shape = (num_samples, k)\n",
    "#         y_test_rescaled.shape = (num_samples, k)\n",
    "\n",
    "k = preds_rescaled.shape[1]\n",
    "step_wise_mae = []\n",
    "\n",
    "for step in range(k):\n",
    "    mae = mean_absolute_error(y_test_rescaled[:, step], preds_rescaled[:, step])\n",
    "    step_wise_mae.append(mae)\n",
    "\n",
    "# In ra t·ª´ng step\n",
    "for i, mae in enumerate(step_wise_mae):\n",
    "    print(f\"Step t+{i+1}: MAE = {mae:.4f} dollars\")\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì MAE theo t·ª´ng b∆∞·ªõc\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, k + 1), step_wise_mae, marker='o')\n",
    "plt.title(\"Step-wise MAE of Predictions\")\n",
    "plt.xlabel(\"Prediction Step (t+1 to t+k)\")\n",
    "plt.ylabel(\"Mean Absolute Error (dollars)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
