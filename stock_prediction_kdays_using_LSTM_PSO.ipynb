{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0639c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # th∆∞ vi·ªán t√≠nh to√°n s·ªë h·ªçc\n",
    "import pandas as pd # th∆∞ vi·ªán gi√∫p ƒë·ªçc file v√† x·ª≠ l√≠ d·ªØ li·ªáu d·∫°ng b·∫£ng\n",
    "import yfinance as yf # th∆∞ vi·ªán l·∫•y d·ªØ li·ªáu\n",
    "import tensorflow as tf # th∆∞ vi·ªán model\n",
    "from tensorflow.keras.models import Sequential # S·∫Øp x·∫øp c√°c l·ªõp\n",
    "from tensorflow.keras.layers import Layer, LSTM, Dense, Dropout # C√°c l·ªõp s·ª≠ d·ª•ng trong m√¥ h√¨nh\n",
    "from sklearn.preprocessing import MinMaxScaler # Chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "from sklearn.metrics import mean_squared_error # T√≠nh ƒë·ªô l·ªói\n",
    "import matplotlib.pyplot as plt # Th∆∞ vi·ªán v·∫Ω ƒë·ªì th·ªã\n",
    "import random # random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba8d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫£i d·ªØ li·ªáu 10 nƒÉm d√πng th∆∞ vi·ªán yfinance\n",
    "def download_stock_data(ticker):\n",
    "    data = yf.download(ticker, period=\"10y\", interval=\"1d\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9812d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• v·ªÅ 1 m√£ c·ªï phi·∫øu\n",
    "data = download_stock_data(\"AAPL\")\n",
    "# ƒê·∫£m b·∫£o d·ªØ li·ªáu l√† c·ªßa c√°c ng√†y li√™n t·ª•c ('D': daily),\n",
    "# c√°c ng√†y kh√¥ng c√≥ d·ªØ li·ªáu (T7,CN) th√¨ gi√° tr·ªã d·ªØ li·ªáu ƒë∆∞·ª£c g√°n NaN\n",
    "data = data.asfreq('D')\n",
    "\n",
    "# Ki·ªÉm tra 10 d√≤ng ƒë·∫ßu ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng s√≥t ng√†y n√†o\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna() l√† h√†m d√πng ƒë·ªÉ ghi ƒë√® c√°c √¥ c√≥ gi√° tr·ªã NaN\n",
    "# method='ffill' (forward fill) nghƒ©a l√†: N·∫øu m·ªôt √¥ c√≥ gi√° tr·ªã NaN,\n",
    "# h√£y l·∫•y gi√° tr·ªã ·ªü d√≤ng ph√≠a tr√™n n√≥ ƒë·ªÉ ƒëi·ªÅn v√†o ()\n",
    "data = data.fillna(method='ffill')\n",
    "\n",
    "# Ki·ªÉm tra 10 d√≤ng ƒë·∫ßu ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≤n gi√° tr·ªã NaN n√†o.\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4020ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y ra duy nh·∫•t c·ªôt gi√° ƒë√≥ng c·ª≠a (Close) ‚Äì\n",
    "# ƒë√¢y l√† d·ªØ li·ªáu quan tr·ªçng nh·∫•t trong ph√¢n t√≠ch t√†i ch√≠nh v√† d·ª± b√°o.\n",
    "close_prices = data[['Close']]\n",
    "\n",
    "# Ki·ªÉm tra 10 gi√° tr·ªã ƒë·∫ßu c·ªßa chu·ªói Close, ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c l·ªçc ƒë√∫ng.\n",
    "close_prices.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a79962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V·∫Ω ƒë·ªì th·ªã gi√° th·ª±c\n",
    "dates = data.index[:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, close_prices, label='Actual Price')\n",
    "plt.title('Stock Price Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120eacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_loss(y_true, y_pred):\n",
    "    diff_true = y_true[:, 1:] - y_true[:, :-1]\n",
    "    diff_pred = y_pred[:, 1:] - y_pred[:, :-1]\n",
    "    return tf.reduce_mean(tf.maximum(0.0, -diff_true * diff_pred))  # penalize opposite direction\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    alpha = 0.4\n",
    "    beta = 0.2  # ph·∫°t underprediction\n",
    "    gamma = 0.3  # ph·∫°t overprediction\n",
    "\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    dir_loss = directional_loss(y_true, y_pred)\n",
    "\n",
    "    under_penalty = tf.reduce_mean(tf.nn.relu(y_true - y_pred))\n",
    "    over_penalty = tf.reduce_mean(tf.nn.relu(y_pred - y_true))\n",
    "\n",
    "    return mse + alpha * dir_loss + beta * under_penalty + gamma * over_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Build LSTM model using library\n",
    "def build_model(input_shape, units=50, output_steps=5):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(output_steps))  # Output shape = (batch_size, k)\n",
    "    model.compile(optimizer='adam', loss= combined_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def create_dataset(data, window_size=60, k=5):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data) - k + 1):  # ensure enough room for k steps\n",
    "        X.append(data[i - window_size:i])\n",
    "        y.append(data[i:i + k].flatten())  # output is a sequence of k steps\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ca71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y d·ªØ li·ªáu g·ªëc\n",
    "close_prices = data['Close'].values.reshape(-1, 1)\n",
    "total_len = len(close_prices)\n",
    "\n",
    "# Chia theo 80% train, 10% val, 10% test\n",
    "train_end = int(0.8 * total_len)\n",
    "val_end = int(0.9 * total_len)\n",
    "\n",
    "# ‚ö†Ô∏è Gi·ªØ l·∫°i 90 ng√†y tr∆∞·ªõc khi chia ƒë·ªÉ ƒë·ªß cho m·ªçi window_size\n",
    "max_window_size = 180\n",
    "train_raw = close_prices[:train_end]\n",
    "val_raw = close_prices[train_end - max_window_size:val_end]\n",
    "test_raw = close_prices[val_end - max_window_size:]\n",
    "\n",
    "# Chu·∫©n h√≥a\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_raw)\n",
    "val_scaled = scaler.transform(val_raw)\n",
    "test_scaled = scaler.transform(test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2086f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_swarm_optimization(population_size=20, max_generations=30,patience=5, target_mse=0.001, no_improve_count=0, w_max=0.9, w_min = 0.4, k=5):\n",
    "    # 1. T·∫°o c√° th·ªÉ ng·∫´u nhi√™n (t·ªça ƒë·ªô v√† v·∫≠n t·ªëc)\n",
    "    def create_particle():\n",
    "        position = {\n",
    "            'window_size': random.randint(30, 180),\n",
    "            'units': random.choice([64, 128])\n",
    "        }\n",
    "        velocity = {\n",
    "            'window_size': random.uniform(-10, 10),\n",
    "            'units': random.choice([-32, 0, 32])\n",
    "        }\n",
    "        return {\n",
    "            'position': position,\n",
    "            'velocity': velocity,\n",
    "            'best_position': position.copy(),\n",
    "            'best_score': float('inf')\n",
    "        }\n",
    "\n",
    "    # 2. T√≠nh fitness\n",
    "    def fitness(ind):\n",
    "        window_size = int(round(ind['position']['window_size']))\n",
    "        units = int(ind['position']['units'])\n",
    "\n",
    "        # Gi·ªõi h·∫°n window_size n·∫±m trong [30, 90] v√† units n·∫±m trong {32, 64, 128}\n",
    "        window_size = min(90, max(30, window_size))\n",
    "        units = min([64, 128], key=lambda x: abs(x - units))\n",
    "\n",
    "        # T·∫°o dataset t·ª´ t·∫≠p ƒë√£ chia & chu·∫©n h√≥a\n",
    "        X_train, y_train = create_dataset(train_scaled, window_size, k)\n",
    "        X_val, y_val = create_dataset(val_scaled, window_size, k)\n",
    "\n",
    "        model = build_model((window_size, 1), units=units, output_steps=k)\n",
    "        model.fit(X_train, y_train, epochs=3, batch_size=32, verbose=0)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mse = mean_squared_error(y_val, preds)\n",
    "        return mse, window_size, units\n",
    "\n",
    "\n",
    "    # 3. Kh·ªüi t·∫°o qu·∫ßn th·ªÉ\n",
    "    swarm = [create_particle() for _ in range(population_size)]\n",
    "    global_best_position = None\n",
    "    global_best_score = float('inf')\n",
    "    best_score_history = float('inf')\n",
    "\n",
    "    for gen in range(max_generations):\n",
    "        print(f\"\\nLoop {gen+1}\")\n",
    "        w = w_max - (w_max - w_min) * gen / max_generations\n",
    "        c1 = 2.5 - 2 * (gen / max_generations)\n",
    "        c2 = 0.5 + 2 * (gen / max_generations)\n",
    "        for i, particle in enumerate(swarm):\n",
    "            score, actual_ws, actual_units = fitness(particle)\n",
    "            print(f\"Particle {i+1}: window_size={actual_ws}, units={actual_units}, MSE={score:.6f}\")\n",
    "\n",
    "            if score < particle['best_score']:\n",
    "                particle['best_score'] = score\n",
    "                particle['best_position'] = particle['position'].copy()\n",
    "\n",
    "            if score < global_best_score:\n",
    "                global_best_score = score\n",
    "                global_best_position = particle['position'].copy()\n",
    "\n",
    "        # 4. C·∫≠p nh·∫≠t v·∫≠n t·ªëc v√† v·ªã tr√≠\n",
    "        for particle in swarm:\n",
    "            for key in ['window_size', 'units']:\n",
    "                r1 = random.random()\n",
    "                r2 = random.random()\n",
    "                cognitive = c1 * r1 * (particle['best_position'][key] - particle['position'][key])\n",
    "                social = c2 * r2 * (global_best_position[key] - particle['position'][key])\n",
    "                particle['velocity'][key] = w * particle['velocity'][key] + cognitive + social\n",
    "                #particle['position'][key] += particle['velocity'][key]\n",
    "\n",
    "                # üëâ Velocity Clamping (Gi·ªõi h·∫°n v·∫≠n t·ªëc)\n",
    "                #v_max = {'window_size': 10, 'units': 32}\n",
    "                #particle['velocity'][key] = max(-v_max[key], min(v_max[key], particle['velocity'][key]))\n",
    "\n",
    "                particle['position'][key] += particle['velocity'][key]\n",
    "                # Gi·ªõi h·∫°n window_size v√† units\n",
    "                if key == 'window_size':\n",
    "                    particle['position'][key] = min(180, max(30, particle['position'][key]))\n",
    "                elif key == 'units':\n",
    "                    # Ch·ªâ cho ph√©p g·∫ßn c√°c gi√° tr·ªã 32, 64, 128\n",
    "                    raw_units = particle['position'][key]\n",
    "                    particle['position'][key] = min([64, 128], key=lambda x: abs(x - raw_units))\n",
    "                    \n",
    "        print(f\"Best individual of generation {gen+1}: window_size={int(global_best_position['window_size'])}, units={int(global_best_position['units'])}, MSE={global_best_score:.6f}\")\n",
    "\n",
    "         # üî∏ ƒêi·ªÅu ki·ªán d·ª´ng s·ªõm\n",
    "        if global_best_score < best_score_history:\n",
    "            best_score_history = global_best_score\n",
    "            no_improve_count = 0 #reset b·ªô ƒë·∫øm\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "\n",
    "        if global_best_score <= target_mse:\n",
    "            print(f\"Early stopping: ƒë·∫°t MSE m·ª•c ti√™u {target_mse} ·ªü th·∫ø h·ªá {gen+1}.\")\n",
    "            break\n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"Early stopping: kh√¥ng c·∫£i thi·ªán trong {patience} th·∫ø h·ªá.\")\n",
    "            break\n",
    "\n",
    "    # Tr·∫£ v·ªÅ k·∫øt qu·∫£ t·ªët nh·∫•t\n",
    "    final_window_size = int(round(global_best_position['window_size']))\n",
    "    final_units = int(global_best_position['units'])\n",
    "    print(f\"\\nFinal best individual: window_size={final_window_size}, units={final_units}, MSE={global_best_score:.6f}\")\n",
    "    return {'window_size': final_window_size, 'units': final_units}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e843eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # GA optimization\n",
    "best_params = particle_swarm_optimization(k=5)\n",
    "print(\"Best Parameters from PSO:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a630a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. L·∫•y d·ªØ li·ªáu g·ªëc (ch∆∞a chu·∫©n h√≥a)\n",
    "close_prices = data['Close'].values.reshape(-1, 1)\n",
    "\n",
    "# 2. Chia d·ªØ li·ªáu theo t·ªâ l·ªá 80% train, 10% val, 10% test\n",
    "total_len = len(close_prices)\n",
    "train_end = int(total_len * 0.8)\n",
    "val_end = int(total_len * 0.9)\n",
    "\n",
    "train_prices = close_prices[:train_end]\n",
    "val_prices = close_prices[train_end - best_params['window_size']:val_end]\n",
    "test_prices = close_prices[val_end - best_params['window_size']:]  # gi·ªØ l·∫°i window_size ng√†y\n",
    "\n",
    "# 3. Fit scaler tr√™n t·∫≠p train v√† transform c·∫£ 3 ph·∫ßn\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train = scaler.fit_transform(train_prices)\n",
    "scaled_val = scaler.transform(val_prices)\n",
    "scaled_test = scaler.transform(test_prices)\n",
    "\n",
    "# 4. T·∫°o dataset\n",
    "k=5\n",
    "X_train, y_train = create_dataset(scaled_train, best_params['window_size'], k)\n",
    "X_val, y_val = create_dataset(scaled_val, best_params['window_size'], k)\n",
    "X_test, y_test = create_dataset(scaled_test, best_params['window_size'], k)\n",
    "\n",
    "# 5. G·ªôp train + val ƒë·ªÉ train m√¥ h√¨nh cu·ªëi c√πng\n",
    "X_final_train = np.concatenate([X_train, X_val])\n",
    "y_final_train = np.concatenate([y_train, y_val])\n",
    "\n",
    "# 6. X√¢y m√¥ h√¨nh\n",
    "model = build_model((best_params['window_size'], 1), best_params['units'])\n",
    "\n",
    "# 7. EarlyStopping (monitor tr√™n `loss` v√¨ kh√¥ng c√≥ val)\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 8. Hu·∫•n luy·ªán tr√™n train + val\n",
    "model.fit(\n",
    "    X_final_train, y_final_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 9. D·ª± ƒëo√°n tr√™n test set v√† t√≠nh RMSE\n",
    "preds = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"‚úÖ Test RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "val_preds = model.predict(X_val)\n",
    "\n",
    "def plot_all_forecasts(data, scaler, best_params, y_val, val_preds, y_test, test_preds):\n",
    "    window_size = best_params['window_size']\n",
    "    k = y_val.shape[1]\n",
    "\n",
    "    total_len = len(data)\n",
    "    train_end = int(0.8 * total_len)\n",
    "    val_end = int(0.9 * total_len)\n",
    "\n",
    "    # --- 1. L·∫•y index g·ªëc ---\n",
    "    train_dates = data.index[:train_end]\n",
    "\n",
    "    val_start_index = train_end - window_size\n",
    "    test_start_index = val_end - window_size\n",
    "\n",
    "    # --- 2. Gi·∫£i scale ---\n",
    "    y_val_rescaled = scaler.inverse_transform(y_val.reshape(-1, 1)).reshape(y_val.shape)\n",
    "    val_preds_rescaled = scaler.inverse_transform(val_preds.reshape(-1, 1)).reshape(val_preds.shape)\n",
    "\n",
    "    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "    test_preds_rescaled = scaler.inverse_transform(test_preds.reshape(-1, 1)).reshape(test_preds.shape)\n",
    "\n",
    "    # --- 3. T·∫°o dict gi√° th·ª±c ---\n",
    "    def get_actual_dict(y_rescaled, start_idx):\n",
    "        actual_dict = defaultdict(list)\n",
    "        for i in range(len(y_rescaled)):\n",
    "            start = start_idx + i + window_size\n",
    "            for j in range(y_rescaled.shape[1]):\n",
    "                date_idx = start + j\n",
    "                if date_idx < len(data):\n",
    "                    date = data.index[date_idx]\n",
    "                    actual_dict[date].append(y_rescaled[i][j])\n",
    "        return actual_dict\n",
    "\n",
    "    actual_val = get_actual_dict(y_val_rescaled, val_start_index)\n",
    "    actual_test = get_actual_dict(y_test_rescaled, test_start_index)\n",
    "\n",
    "    # --- 4. D·ª± ƒëo√°n t·ª´ng b∆∞·ªõc ---\n",
    "    def get_pred_lines(preds_rescaled, start_idx):\n",
    "        pred_step_dict = [defaultdict(list) for _ in range(k)]\n",
    "        for i in range(len(preds_rescaled)):\n",
    "            start = start_idx + i + window_size\n",
    "            for j in range(k):\n",
    "                date_idx = start + j\n",
    "                if date_idx < len(data):\n",
    "                    date = data.index[date_idx]\n",
    "                    pred_step_dict[j][date].append(preds_rescaled[i][j])\n",
    "        pred_lines = []\n",
    "        for j in range(k):\n",
    "            common_dates = sorted(set(pred_step_dict[j].keys()))\n",
    "            pred_avg = np.array([np.mean(pred_step_dict[j][d]) for d in common_dates])\n",
    "            pred_lines.append((common_dates, pred_avg))\n",
    "        return pred_lines\n",
    "\n",
    "    val_pred_lines = get_pred_lines(val_preds_rescaled, val_start_index)\n",
    "    test_pred_lines = get_pred_lines(test_preds_rescaled, test_start_index)\n",
    "\n",
    "    # --- 5. Plot ---\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Train th·ª±c t·∫ø\n",
    "    plt.plot(train_dates, data['Close'][:train_end], label='Train Actual', color='gray', linewidth=1.5)\n",
    "\n",
    "    # Val th·ª±c t·∫ø\n",
    "    val_actual_dates = sorted(actual_val.keys())\n",
    "    val_actual_avg = np.array([np.mean(actual_val[d]) for d in val_actual_dates])\n",
    "    plt.plot(val_actual_dates, val_actual_avg, label='Val Actual', color='blue', linewidth=2)\n",
    "\n",
    "    # Test th·ª±c t·∫ø\n",
    "    test_actual_dates = sorted(actual_test.keys())\n",
    "    test_actual_avg = np.array([np.mean(actual_test[d]) for d in test_actual_dates])\n",
    "    plt.plot(test_actual_dates, test_actual_avg, label='Test Actual', color='black', linewidth=2)\n",
    "\n",
    "    # C√°c b∆∞·ªõc d·ª± ƒëo√°n val\n",
    "    colors = plt.cm.Blues(np.linspace(0.4, 0.9, k))\n",
    "    for j, (dates, preds_j) in enumerate(val_pred_lines):\n",
    "        plt.plot(dates, preds_j, label=f'Val Predicted t+{j+1}', color=colors[j], linestyle='--')\n",
    "\n",
    "    # C√°c b∆∞·ªõc d·ª± ƒëo√°n test\n",
    "    colors = plt.cm.Greens(np.linspace(0.4, 0.9, k))\n",
    "    for j, (dates, preds_j) in enumerate(test_pred_lines):\n",
    "        plt.plot(dates, preds_j, label=f'Test Predicted t+{j+1}', color=colors[j], linestyle='--')\n",
    "\n",
    "    plt.title(f'Stock Forecasting: Actual vs {k}-step Predictions')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_all_forecasts(data, scaler, best_params, y_val, val_preds, y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D·ª± ƒëo√°n tr√™n t·∫≠p validation\n",
    "val_preds = model.predict(X_val)\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Rescale val ground truth v√† d·ª± ƒëo√°n\n",
    "y_val_2d = y_val.reshape(-1, 1)\n",
    "val_preds_2d = val_preds.reshape(-1, 1)\n",
    "\n",
    "y_val_rescaled = scaler.inverse_transform(y_val_2d).reshape(y_val.shape)\n",
    "val_preds_rescaled = scaler.inverse_transform(val_preds_2d).reshape(val_preds.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. T·∫°o actual_dict t·ª´ y_val_rescaled\n",
    "actual_dict = defaultdict(list)\n",
    "val_start_index = len(data) - len(y_test) - len(y_val) - best_params['window_size']\n",
    "\n",
    "for i in range(len(y_val_rescaled)):\n",
    "    start = val_start_index + i + best_params['window_size']\n",
    "    for j in range(y_val_rescaled.shape[1]):\n",
    "        date_idx = start + j\n",
    "        if date_idx < len(data):\n",
    "            date = data.index[date_idx]\n",
    "            actual_dict[date].append(y_val_rescaled[i][j])\n",
    "\n",
    "actual_dates = sorted(actual_dict.keys())\n",
    "actual_avg = np.array([np.mean(actual_dict[d]) for d in actual_dates])\n",
    "\n",
    "# -------------------------------\n",
    "# 3. D·ª± ƒëo√°n t·ª´ng b∆∞·ªõc t+1, ..., t+k\n",
    "k = val_preds_rescaled.shape[1]\n",
    "pred_step_dict = [defaultdict(list) for _ in range(k)]\n",
    "\n",
    "for i in range(len(val_preds_rescaled)):\n",
    "    start = val_start_index + i + best_params['window_size']\n",
    "    for j in range(k):\n",
    "        date_idx = start + j\n",
    "        if date_idx < len(data):\n",
    "            date = data.index[date_idx]\n",
    "            pred_step_dict[j][date].append(val_preds_rescaled[i][j])\n",
    "\n",
    "# Trung b√¨nh t·ª´ng b∆∞·ªõc\n",
    "pred_lines = []\n",
    "step_mae = []\n",
    "for j in range(k):\n",
    "    common_dates = sorted(set(pred_step_dict[j].keys()) & set(actual_dict.keys()))\n",
    "    pred_avg = np.array([np.mean(pred_step_dict[j][d]) for d in common_dates])\n",
    "    actual_avg_step = np.array([np.mean(actual_dict[d]) for d in common_dates])\n",
    "    pred_lines.append((common_dates, pred_avg))\n",
    "    step_mae.append(np.mean(np.abs(pred_avg - actual_avg_step)))\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# ƒê∆∞·ªùng th·ª±c t·∫ø\n",
    "plt.plot(actual_dates, actual_avg, label='Actual Price', color='black', linewidth=2)\n",
    "\n",
    "# C√°c ƒë∆∞·ªùng d·ª± ƒëo√°n t+1 ƒë·∫øn t+k\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, k))\n",
    "for j in range(k):\n",
    "    dates, preds_j = pred_lines[j]\n",
    "    plt.plot(dates, preds_j, label=f'Predicted t+{j+1}', color=colors[j], alpha=0.7)\n",
    "\n",
    "plt.title(f'Many-to-Many Validation Forecasting: Actual vs {k} Prediction Steps')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#n ng√†y ƒë·ªÉ d·ª± ƒëo√°n k ng√†y (n+1 ƒë∆∞·ª£c d·ª± ƒëo√°n b·ªüi n ng√†y, n+2 ƒë∆∞·ª£c d·ª± ƒëo√°n b·ªüi n ng√†y)\n",
    "#(window_size, units) -> h∆°i kh√≥\n",
    "# Reshape cho inverse transform\n",
    "y_test_2d = y_test.reshape(-1, 1)\n",
    "preds_2d = preds.reshape(-1, 1)\n",
    "\n",
    "y_test_rescaled = scaler.inverse_transform(y_test_2d).reshape(y_test.shape)\n",
    "preds_rescaled = scaler.inverse_transform(preds_2d).reshape(preds.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# Chu·∫©n b·ªã actual price (trung b√¨nh n·∫øu tr√πng)\n",
    "actual_dict = defaultdict(list)\n",
    "start_index = len(data) - len(y_test_rescaled) - best_params['window_size']\n",
    "\n",
    "for i in range(len(y_test_rescaled)):\n",
    "    start = start_index + i + best_params['window_size']\n",
    "    for j in range(y_test_rescaled.shape[1]):\n",
    "        date_idx = start + j\n",
    "        if date_idx < len(data):\n",
    "            date = data.index[date_idx]\n",
    "            actual_dict[date].append(y_test_rescaled[i][j])\n",
    "\n",
    "# Trung b√¨nh gi√° th·ª±c t·∫ø n·∫øu tr√πng\n",
    "actual_dates = sorted(actual_dict.keys())\n",
    "actual_avg = np.array([np.mean(actual_dict[d]) for d in actual_dates])\n",
    "\n",
    "# ----------------------------\n",
    "# D·ª± ƒëo√°n theo t·ª´ng b∆∞·ªõc t+1, t+2, ..., t+k\n",
    "k = preds_rescaled.shape[1]\n",
    "pred_step_dict = [defaultdict(list) for _ in range(k)]\n",
    "\n",
    "for i in range(len(preds_rescaled)):\n",
    "    start = start_index + i + best_params['window_size']\n",
    "    for j in range(k):\n",
    "        date_idx = start + j\n",
    "        if date_idx < len(data):\n",
    "            date = data.index[date_idx]\n",
    "            pred_step_dict[j][date].append(preds_rescaled[i][j])\n",
    "\n",
    "# Trung b√¨nh t·ª´ng b∆∞·ªõc + t√≠nh MAE\n",
    "pred_lines = []\n",
    "step_mae = []\n",
    "for j in range(k):\n",
    "    common_dates = sorted(set(pred_step_dict[j].keys()) & set(actual_dict.keys()))\n",
    "    pred_avg = np.array([np.mean(pred_step_dict[j][d]) for d in common_dates])\n",
    "    actual_avg_step = np.array([np.mean(actual_dict[d]) for d in common_dates])\n",
    "    pred_lines.append((common_dates, pred_avg))\n",
    "    step_mae.append(np.mean(np.abs(pred_avg - actual_avg_step)))\n",
    "\n",
    "# ----------------------------\n",
    "# Trung b√¨nh t·∫•t c·∫£ b∆∞·ªõc d·ª± ƒëo√°n\n",
    "combined_pred_dict = defaultdict(list)\n",
    "for j in range(k):\n",
    "    for date, values in pred_step_dict[j].items():\n",
    "        combined_pred_dict[date].extend(values)\n",
    "\n",
    "common_avg_dates = sorted(set(combined_pred_dict.keys()) & set(actual_dict.keys()))\n",
    "avg_pred_line = np.array([np.mean(combined_pred_dict[d]) for d in common_avg_dates])\n",
    "\n",
    "# ----------------------------\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# ƒê∆∞·ªùng gi√° th·ª±c t·∫ø\n",
    "plt.plot(actual_dates, actual_avg, label='Actual Price', linewidth=2)\n",
    "\n",
    "# C√°c ƒë∆∞·ªùng d·ª± ƒëo√°n t+1 ƒë·∫øn t+k\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, k))\n",
    "for j in range(k):\n",
    "    dates, preds_j = pred_lines[j]\n",
    "    plt.plot(dates, preds_j, label=f'Predicted t+{j+1}', color=colors[j], alpha=0.7, linewidth=2)\n",
    "\n",
    "# ƒê∆∞·ªùng trung b√¨nh c√°c b∆∞·ªõc d·ª± ƒëo√°n\n",
    "plt.plot(common_avg_dates, avg_pred_line, label='Average Prediction', color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.title(f'Many-to-Many Forecasting: Actual vs {k} Prediction Steps')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≠nh MAE v√† MAPE cho ƒë∆∞·ªùng d·ª± ƒëo√°n trung b√¨nh\n",
    "common_actual_avg = np.array([np.mean(actual_dict[d]) for d in common_avg_dates])\n",
    "\n",
    "# MAE: sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh\n",
    "avg_mae = np.mean(np.abs(avg_pred_line - common_actual_avg))\n",
    "\n",
    "# MAPE: sai s·ªë ph·∫ßn trƒÉm tuy·ªát ƒë·ªëi trung b√¨nh\n",
    "# Tr√°nh chia cho 0 b·∫±ng c√°ch th√™m epsilon nh·ªè\n",
    "epsilon = 1e-8\n",
    "avg_mape = np.mean(np.abs((common_actual_avg - avg_pred_line) / (common_actual_avg + epsilon))) * 100\n",
    "\n",
    "print(f\"\\n‚û°Ô∏è  MAE (Average Prediction): {avg_mae:.4f} USD\")\n",
    "print(f\"‚û°Ô∏è  MAPE (Average Prediction): {avg_mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# preds, y_test ƒë√£ rescale r·ªìi (preds_rescaled, y_test_rescaled)\n",
    "# Gi·∫£ s·ª≠: preds_rescaled.shape = (num_samples, k)\n",
    "#         y_test_rescaled.shape = (num_samples, k)\n",
    "\n",
    "k = preds_rescaled.shape[1]\n",
    "step_wise_mae = []\n",
    "\n",
    "for step in range(k):\n",
    "    mae = mean_absolute_error(y_test_rescaled[:, step], preds_rescaled[:, step])\n",
    "    step_wise_mae.append(mae)\n",
    "\n",
    "# In ra t·ª´ng step\n",
    "for i, mae in enumerate(step_wise_mae):\n",
    "    print(f\"Step t+{i+1}: MAE = {mae:.4f} dollars\")\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì MAE theo t·ª´ng b∆∞·ªõc\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, k + 1), step_wise_mae, marker='o')\n",
    "plt.title(\"Step-wise MAE of Predictions\")\n",
    "plt.xlabel(\"Prediction Step (t+1 to t+k)\")\n",
    "plt.ylabel(\"Mean Absolute Error (dollars)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
